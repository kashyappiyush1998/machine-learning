## USE ANACONDA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv("/home/piyush/Documents/quora ml/input/input00.csv",header=None)
df.head()
for i in df.columns:
    if i>0 and i%2==0:
        df.drop(i,inplace=True,axis=1)
df.drop([0,3,45,47,39],inplace=True,axis=1)
df.head()
##drop 39 because we find feature importance 0 in dec tree
##0,3 are drpped because all values are different
for i in df.columns:
    print(df[i].nunique())
X = df.iloc[:,1:]
X.head()
y = df[1]
y.ndim
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
dt = DecisionTreeClassifier()
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
dt.fit(X_train,y_train)
dt.score(X_train,y_train)
dt.score(X_test,y_test)
dt.fit(X,y)
##fi = dt.feature_importances_
dt.feature_importances_
##this indicates there is outlier on looking at given data we found out that in few colums all 4500 values are different
##drop them
dt.score(X,y)
test = pd.read_csv("/home/piyush/Documents/quora ml/test.csv",header=None)
test.head()
test.drop([0,1,22,23,19],inplace=True,axis=1)
test.head()
pred = dt.predict(test)
pred
##By Rnadom Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
rf = RandomForestClassifier()
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
rf.fit(X_train,y_train)
rf.score(X_train,y_train)
rf.score(X_test,y_test)
## By Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train,y_train)
lr.score(X_train,y_train)
lr.score(X_test,y_test)
## SVM Classifier
from sklearn.svm import SVC
sv =SVC(kernel="linear",probability=True)
sv.fit(X_train,y_train)
sv.score(X_train,y_train)
sv.score(X_test,y_test)
